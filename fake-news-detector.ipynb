{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (1.21.36)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (from boto3) (0.5.2)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (from boto3) (1.0.0)\r\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.36 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (from boto3) (1.24.36)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.36->boto3) (2.8.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.36->boto3) (1.26.7)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/aleemullahkhan/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.36->boto3) (1.16.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 22.0.2; however, version 22.0.4 is available.\r\n",
      "You should consider upgrading via the '/Users/aleemullahkhan/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleemullahkhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleemullahkhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "import os\n",
    "!pip install boto3\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import read_data, process_text, prepare_data, extract_features\n",
    "\n",
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "# from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "# from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Set global variables\n",
    "RANDOM_STATE = 5\n",
    "DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/True.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/83/8gchw24s00v8m6yk0fmzctnr0000gn/T/ipykernel_17754/4219397596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_STATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Show first rows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/fake-news-detector-master/helpers.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(path, random_state)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \"\"\"\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Read data in pandas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"True.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Fake.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/True.csv'"
     ]
    }
   ],
   "source": [
    "# Read data.\n",
    "df = read_data(DIR, RANDOM_STATE)\n",
    "\n",
    "# Show first rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pygmentize helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join title and text into one column.\n",
    "df['text'] = df.title + \" \" + df.text\n",
    "\n",
    "# Remove useless columns.\n",
    "df.drop(columns=['subject', 'date', 'title'], axis=1, inplace=True)\n",
    "\n",
    "# Show the first rows.\n",
    "display(df.head())\n",
    "\n",
    "# Show an example of text.\n",
    "df.text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to train and test datasets.\n",
    "train_X, test_X, train_y, test_y = train_test_split(df.text, df.label, test_size=0.2,\n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Fake and True News (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[3])\n",
    "print(train_y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply process_text to an example.\n",
    "process_text(train_X[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'tqdm' instance to time and estimate the progress of functions.\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure directory exists.\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "# Preprocess data.\n",
    "train_X, test_X, train_y, test_y = prepare_data(train_X, test_X, train_y, test_y, cache_dir=DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(train_X[5])\n",
    "print(len(train_X[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some n-grams from the dictionary.\n",
    "for key in sorted(vocabulary, key=vocabulary.get, reverse=True)[:20]:\n",
    "    print(key, ':', vocabulary[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit the model.\n",
    "model = MultinomialNB()\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and save the predictions.\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "print(confusion_matrix(test_y, predictions))\n",
    "print(classification_report(test_y, predictions))\n",
    "plot_confusion_matrix(model, test_X, test_y, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to train, validation, and test datasets.\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Put label in first column.\n",
    "df_train = df_train[['label', 'text']]\n",
    "df_valid = df_valid[['label', 'text']]\n",
    "df_test = df_test[['label', 'text']]\n",
    "\n",
    "# Add __label__ to class as prefix.\n",
    "df_train.label = '__label__' + df_train.label.astype('str')\n",
    "df_valid.label = '__label__' + df_valid.label.astype('str')\n",
    "df_test.label = '__label__' + df_test.label.astype('str')\n",
    "\n",
    "# Clean and normalize text.\n",
    "df_train.text = df_train.text.progress_apply(process_text)\n",
    "df_valid.text = df_valid.text.progress_apply(process_text)\n",
    "df_test.text = df_test.text.progress_apply(process_text)\n",
    "\n",
    "# Show dfs.\n",
    "display(df_train.head())\n",
    "display(df_valid.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "df_train.to_csv(os.path.join(DIR, 'news.train'), sep=' ', header=False, index=False)\n",
    "df_valid.to_csv(os.path.join(DIR, 'news.valid'), sep=' ', header=False, index=False)\n",
    "df_test.to_csv(os.path.join(DIR, 'news.test'), sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import sagemaker\n",
    "\n",
    "# Store the current SageMaker session.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Store the bucket.\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# S3 prefix (which folder will we use).\n",
    "prefix = 'fake-news-bt'\n",
    "\n",
    "# Upload the processed test, train and validation files,\n",
    "# which are contained in data directory to S3 using session.upload_data().\n",
    "test_location = session.upload_data(os.path.join(DIR, 'news.test'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(DIR, 'news.valid'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(DIR, 'news.train'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our current execution role is required when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to retrieve the location of the container, which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "container = get_image_uri(session.boto_region_name, 'blazingtext', 'latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a SageMaker estimator object for our model.\n",
    "bt_model = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                         role, # What is our current IAM Role\n",
    "                                         train_instance_count=1, # How many compute instances\n",
    "                                         train_instance_type='ml.c4.4xlarge', # What kind of compute instances\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                         sagemaker_session=session)\n",
    "\n",
    "# And then set the algorithm specific parameters.\n",
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                             epochs=10,\n",
    "                             min_count=2,\n",
    "                             learning_rate=0.05,\n",
    "                             vector_dim=10,\n",
    "                             early_stopping=True,\n",
    "                             patience=4,\n",
    "                             min_epochs=5,\n",
    "                             word_ngrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, distribution='FullyReplicated',\n",
    "                                    content_type='text/plain')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, distribution='FullyReplicated',\n",
    "                                         content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(os.path.join(DIR, 'news.test'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to define the batches.\n",
    "# From: https://stackoverflow.com/questions/8290397/how-to-split-an-iterable-in-constant-size-chunks\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches of 512 inputs for prediction and add them to a list.\n",
    "predictions = []\n",
    "for x in batch(test_X.iloc[:,0].str[12:-1].tolist(), 512):\n",
    "    payload = {\"instances\" : x}\n",
    "    prediction_batch = predictor.predict(json.dumps(payload))\n",
    "    prediction_batch = [int(prediction.get(\"label\")[0][9:]) for prediction in json.loads(prediction_batch)]\n",
    "    predictions.append(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten list.\n",
    "predictions = sum(predictions, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean only if you don't want to use later the Lambda function and API\n",
    "# to make predictions through a simple web app.\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "# And we need the following libraries to do some of the data processing.\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# Create a function to process text.\n",
    "def process_text(text):\n",
    "    # Normalize links replacing them with the str 'link'.\n",
    "    text = re.sub('http\\S+', 'link', text)\n",
    "\n",
    "    # Normalize numbers replacing them with the str 'number'.\n",
    "    text = re.sub('\\d+', 'number', text)\n",
    "\n",
    "    # Normalize emails replacing them with the str 'email'.\n",
    "    text = re.sub('\\S+@\\S+', 'email', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove punctuation.    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove whitespaces.\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Convert all letters to lower case.\n",
    "    text = text.lower()\n",
    "\n",
    "    # Split text into words.\n",
    "    words = text.split()\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    data = event['body']\n",
    "    sentence = process_text(data)\n",
    "\n",
    "    try:\n",
    "        payload = {\"instances\" : sentence}\n",
    "\n",
    "        response = runtime.invoke_endpoint(EndpointName='blazingtext-2020-06-15-07-32-18-142',\n",
    "                                            ContentType='application/json',\n",
    "                                            Body=json.dumps(payload))\n",
    "\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        # prob = []\n",
    "        labels = []\n",
    "        for label in result[0]['label']:\n",
    "            labels.append(label[9:])\n",
    "        print(\"DATA\", data)\n",
    "        print(\"SENTENCE\", sentence)\n",
    "        return {\n",
    "            'statusCode' : 200,\n",
    "            'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "            'body' : str(labels[0])\n",
    "    }\n",
    "        # return {'statusCode': 200, 'body': str(labels[0])}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {'statusCode': 400,\n",
    "                'body': json.dumps({'error_message': 'Unable to generate tag.'})}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data directory.\n",
    "!rm $data_dir/*\n",
    "\n",
    "# Then we delete the directory itself.\n",
    "!rmdir $data_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
